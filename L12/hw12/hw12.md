### 1.LM算法相关
*1.1 绘制阻尼因子$\mu$随迭代变化曲线图*
![lambda_iteration](./lambda_iteration.png)

    # 绘制变化曲线的python程序
    import matplotlib.pyplot as plt

    x_list = list(range(0, 12))
    lambda_list = [0.001, 699.051, 1864.14, 1242.76, 
        414.252, 138.084, 46.028, 15.3427, 5.11423, 1.70474, 0.568247, 0.378832]

    plt.figure('lambda iteration')
    ax = plt.gca()

    ax.set_xlabel('x')
    ax.set_ylabel('lambda')

    ax.scatter(x_list, lambda_list, c='r', s=20, alpha=0.5)
    ax.plot(x_list, lambda_list, c='g', linewidth=1, alpha=0.6)

    plt.show()

*1.2 改曲线函数，修改代码中残差计算，实现曲线参数估计*
修改CurveFitting.cpp代码中部分如下：

    // main函数部分：
    double x = i / 100.;
    double n = noise(generator);
    // 观测 y
    // double y = std::exp( a*x*x + b*x + c ) + n;
    double y = a * x * x + b * x + c + n;

    // ComputeResidual函数部分：
    Vec3 abc = verticies_[0]->Parameters();                      // 估计的参数
    residual_(0) = abc(0) * x_ * x_ + abc(1) * x_ + abc(2) - y_; // 构建残差

    // ComputeJacobians函数部分：
    Eigen::Matrix<double, 1, 3> jaco_abc; // 误差为1维，状态量 3 个，所以是 1x3 的雅克比矩阵
    jaco_abc << x_ * x_, x_, 1;
    jacobians_[0] = jaco_abc;

*（一）观测数量的讨论：*
同学们可以尝试一下采样点数N取不同值，看看拟合参数结果会如何变化。当N较小比如取100时，拟合出来a=1.61039，b=1.61853，c=0.995178，和真值相差较大。当N取较大时，拟合出来参数会逐渐收敛于真值。
比如，将采样数据点N增大为1000，得到结果如下：
![curve_fitting](./curve_fitting.png)
*（二）初值的讨论：*
再次取1000个点，这次考虑将初值(0.0 0.0 0.0)修改为(0.9 2.1 0.9)，即给待估计参数一个较好的初值，再次运行结果如下。与上面的1000点的结果对比，发现参数估计精度没有提升。推测原因是：我们拟合的曲线函数较简单，且只有一个极小值点，因此初值的选择并不会使得优化至错误的极小值，所以最终的精度更多地取决于观测数据的噪声。但是，较好的初值带来的好处便是，迭代次数的减少、耗时的减少，当然更快地收敛。
在面对真正复杂的函数时，好的初始值还是很有必要的。
![curve_fitting1](./curve_fitting1.png)


*1.3 实现更优秀的阻尼因子策略，给出实验对比*
论文《The Levenberg-Marquardt method for nonlinear least squares curve-fitting problems》中有三种阻尼因子策略，如下图所示：
![mu](./mu_iteration.png)
原代码中采用的是第3种策略，即Nielsen策略。下面在原代码problem.cc基础上实现论文中第1种阻尼因子更新策略，修改代码为：

    void Problem::ComputeLambdaInitLM()
    {
        currentChi_ = 0.0;
        // TODO:: robust cost chi2
        for (auto edge : edges_)
        {
            currentChi_ += edge.second->Chi2();
        }
        if (err_prior_.rows() > 0)
            currentChi_ += err_prior_.norm();

        stopThresholdLM_ = 1e-6 * currentChi_; // 迭代条件为 误差下降 1e-6 倍
        currentLambda_ = 1e-3;
    }

    void Problem::AddLambdatoHessianLM()
    {
        ulong size = Hessian_.cols();
        assert(Hessian_.rows() == Hessian_.cols() && "Hessian is not square");
        for (ulong i = 0; i < size; ++i)
        {
            Hessian_(i, i) += currentLambda_ * Hessian_(i, i);
        }
    }

    void Problem::RemoveLambdaHessianLM()
    {
        ulong size = Hessian_.cols();
        assert(Hessian_.rows() == Hessian_.cols() && "Hessian is not square");
        for (ulong i = 0; i < size; ++i)
        {
            Hessian_(i, i) /= 1.0 + currentLambda_;
        }
    }

    bool Problem::IsGoodStepInLM()
    {
        // 统计所有的残差
        double tempChi = 0.0;
        for (auto edge : edges_)
        {
            edge.second->ComputeResidual();
            tempChi += edge.second->Chi2();
        }
        // compute rho
        assert(Hessian_.rows() == Hessian_.cols() && "Hessian is not square");
        ulong size = Hessian_.cols();
        MatXX diag_hessian(MatXX::Zero(size, size));
        for (ulong i = 0; i < size; ++i)
        {
            diag_hessian(i, i) = Hessian_(i, i);
        }
        double scale = delta_x_.transpose() * 
            (currentLambda_ * diag_hessian * delta_x_ + b_);
        double rho = (currentChi_ - tempChi) / scale;
        // update currentLambda_
        double epsilon = 0.0;
        double L_down = 9.0;
        double L_up = 11.0;
        if (rho > epsilon && isfinite(tempChi))
        {
            currentLambda_ = std::max(currentLambda_ / L_down, 1e-7);
            currentChi_ = tempChi;
            return true;
        }
        else
        {
            currentLambda_ = std::min(currentLambda_ * L_up, 1e7);
            return false;
        }
    }

代码运行结果为：
![run](./run_1.png)

同学们可以自行比较一下两种方法的精度、迭代次数、运行耗时等性能，在实际运用的过程中合理选择使用哪种方法。

### 2.公式推导
*1.推导f_{15}*
这里需要注意两点：
* 对谁加扰动，扰动项就直接跟在该变量后面
* 只有旋转变量加扰动才是乘一个微小旋转，其他均是加号
推导过程如下：
![2.1](./21.png)

*2.推导g_{12}*
![2.2](./22.png)
### 3.证明式(9)
![3.1](./3.1.png)
![3.2](./3.2.png)

